<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="width=device-width,initial-scale=1.0" name=viewport><base href=/><title>Kafka Consumer Memory Tuning | cnr.sh</title><link href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’¬</text></svg>" rel=icon><link title="Chris Riccomini's RSS Feed" href=/rss.xml rel=alternate type=application/rss+xml><link href=https://fonts.googleapis.com rel=preconnect><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href="https://fonts.googleapis.com/css2?family=Share+Tech+Mono&display=block" rel=stylesheet><link href=/css/style.css rel=stylesheet><link href=/css/pygments.css rel=stylesheet><meta content=summary name=twitter:card><meta content="Kafka Consumer Memory Tuning | cnr.sh" name=twitter:title><meta content="Kafka Consumer Memory Tuning | cnr.sh" property=og:title><meta content=website property=og:type><meta content="Yesterday, I had a process that was consuming a single Kafka topic. I was running it in our " ..." 512 and environment, everything for great. heap my name=twitter:description process set space staging" the to was worked><meta content="Yesterday, I had a process that was consuming a single Kafka topic. I was running it in our " ..." 512 and environment, everything for great. heap my process property=og:description set space staging" the to was worked><body><header><h1><a href=/>cnr.sh</a></h1><nav><ul><li><a href=/talks>Talks</a><li><a href=/404>Oops!</a><li><a href=/essays>Index</a></ul></nav></header><main><p>Yesterday, I had a process that was consuming a single Kafka topic. I was running it in our "staging" environment, and everything worked great. My heap space for the process was set to 512 megabytes (<code>-Xmx512M</code>). When I moved this process to production, my process would fail with an out of memory exception. I was seeing:<pre><code>java.lang.OutOfMemoryError: Java heap space
BoundedByteBufferReceive [ERROR] OOME with size 4800026
java.lang.OutOfMemoryError: GC overhead limit exceeded
FetcherRunnable [ERROR] error in FetcherRunnable
</code></pre><p>Let's review what happened, and how to fix it.<p><em><strong>WARNING: This is for the legacy Java Kafka consumer.</strong></em><h2>Buffers</h2><p>When you create a Kafka consumer, you first instantiate a Kafka connector (<a href=https://github.com/kafka-dev/kafka/blob/master/core/src/main/scala/kafka/consumer/ConsumerConnector.scala>ConsumerConnector.scala</a>). Then, you create multiple threads that feed off of one or more topics:<div class=highlight><pre><span></span><span class=c1>// create 4 partitions of the stream for topic "test", to allow 4 threads to consume</span>
<span class=n>Map</span><span class=o><</span><span class=n>String</span><span class=p>,</span><span class=w> </span><span class=n>List</span><span class=o><</span><span class=n>KafkaStream</span><span class=o><</span><span class=n>Message</span><span class=o>>>></span><span class=w> </span><span class=n>topicMessageStreams</span><span class=w> </span><span class=o>=</span><span class=w> </span>
<span class=w>    </span><span class=n>consumerConnector</span><span class=p>.</span><span class=na>createMessageStreams</span><span class=p>(</span><span class=n>ImmutableMap</span><span class=p>.</span><span class=na>of</span><span class=p>(</span><span class=s>"test"</span><span class=p>,</span><span class=w> </span><span class=mi>4</span><span class=p>));</span>
<span class=n>List</span><span class=o><</span><span class=n>KafkaStream</span><span class=o><</span><span class=n>Message</span><span class=o>>></span><span class=w> </span><span class=n>streams</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>topicMessageStreams</span><span class=p>.</span><span class=na>get</span><span class=p>(</span><span class=s>"test"</span><span class=p>);</span>
</pre></div><p>Internally, Kafka creates a buffer for each thread attached to the ConsumerConnector. In this case, there are four threads, and therefore four buffers. These buffers, which are queues, are populated asynchronously until they are "full". When your code reads from a stream, Kafka dequeues from the stream/thread's queue, and gives you a message.<h2>Tuning memory usage</h2><p>Two important questions arise from this:<ul><li>When are the queues full?<li>What are the queues populated with?</ul><p>A queue is full when it reaches the configured maximum queue size (queuedchunks.max). That is, if queuedchunks.max=10, then the queue will be full when 10 objects are in it.<p>This leads me to question number two: What are these objects that the queue is populated with? It turns out, <em>they are not messages</em>. Instead, they are fetched byte buffers that contain <em>multiple messages</em>. The size of these byte buffers is determined by the configuration parameter: fetch.size.<p>So, to calculate how much memory your consumer is going to take, you have to use this formula:<pre><code>(number of consumer threads) * (queuedchunks.max) * (fetch.size)
</code></pre><p>For example, if you have 24 threads, a max queue size of 10, and a fetch.size of 1.2 megabytes, your consumer is going to take 288 megabytes of heap space (24 threads * 10 fetches * 1.2 megabytes/fetch) if all queues are full.<p>If you run out of space, you have a few options: increase heap space, reduce your consumer threads, or lower your fetch size or max queue size. Obviously, different tunings have different affects on your throughput. With fewer buffers, or fewer fetches per queue, you might negatively impact your throughput.<h2>What happened to my process?</h2><p>The number of threads in my process was dependent on how many partitions the topic had that I was consuming from. When I moved from staging to production, the Kafka cluster I was consuming from had far more brokers, and far more partitions per topic. As a result, the memory footprint of my process drastically changed. I went from 22 threads to 32, which changed my heap usage from 264 megabytes to 384 megabytes. This was enough to set my process' total memory usage over 512 megabytes, which caused the out of memory exceptions.</main>