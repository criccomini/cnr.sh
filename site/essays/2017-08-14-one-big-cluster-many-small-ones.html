<!doctype html><html lang=en><head><meta charset=UTF-8><meta content="width=device-width,initial-scale=1.0" name=viewport><base href=/><title>One big cluster, or many small ones? | cnr.sh</title><link href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’¬</text></svg>" rel=icon><link title="Chris Riccomini's RSS Feed" href=/rss.xml rel=alternate type=application/rss+xml><link href=https://fonts.googleapis.com rel=preconnect><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link href="https://fonts.googleapis.com/css2?family=Share+Tech+Mono&display=block" rel=stylesheet><link href=/css/style.css rel=stylesheet><link href=/css/pygments.css rel=stylesheet><script crossorigin src=https://kit.fontawesome.com/672d96e063.js></script><script src="https://www.googletagmanager.com/gtag/js?id=G-3R91BN0RW0" async></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[];gtag(`js`,new Date());gtag(`config`,`G-3R91BN0RW0`)</script><meta content=summary name=twitter:card><meta content="One big cluster, or many small ones? | cnr.sh" name=twitter:title><meta content="One big cluster, or many small ones? | cnr.sh" property=og:title><meta content=website property=og:type><meta content="I bumped into something recently that seems to recur at every company I work for. Should we run one big cluster, or many smaller ones? The discussion is usually triggered when you have more than ..." name=twitter:description><meta content="I bumped into something recently that seems to recur at every company I work for. Should we run one big cluster, or many smaller ones? The discussion is usually triggered when you have more than ..." property=og:description><body><header><h1><a href=/>cnr.sh</a></h1><nav><ul><li><a href=/talks>Talks</a><li><a href=/404>Oops!</a><li><a href=/essays>Index</a></ul></nav></header><main><p>I bumped into something recently that seems to recur at every company I work for. Should we run one big cluster, or many smaller ones? The discussion is usually triggered when you have more than one team that wants to use the infrastructure in question.<p>The question is, should an organization run a single instance of the infrastructure, or many physically separate instances (sometimes called federation, though itâ€™s an overloaded term). This question holds true for both clustered solutions (<a href=https://kafka.apache.org/>Kafka</a>, <a href=https://hadoop.apache.org/>Hadoop</a>, <a href=https://cassandra.apache.org/>Cassandra</a>, etc.) and more traditional single node infrastructure (<a href=https://www.mysql.com/>MySQL</a>, <a href=https://www.postgresql.org/>Postgres</a>, etc.).<p>My most recent foray into the discussion centered around MySQL deployments, but Iâ€™ve seen the same discussion play out for Hadoop, Kafka, and some other internal database stuff when I was at LinkedIn.<p>People sometimes feel pretty strongly about the right approach. In the end, as usual, itâ€™s a trade off, and the right answer is dependent on a lot of variables. That said, you should consider at least the following when thinking through this problem:<ul><li>Isolation<li>Security<li>Scalability<li>Migration<li>Automation<li>Deployment heterogeneity<li>SLAs<li>Workload<li>Cost<li>Conwayâ€™s law<li>Multi-region</ul><p>Letâ€™s take a look at these.<h2>Isolation</h2><p>How comfortable are you with one user affecting another user on the system? Isolation can include data isolation, package isolation, query isolation, CPU isolation, network isolation, and a lot more. Iâ€™m going to split isolation into several categories: performance, security, and deployment.<h3>Performance</h3><p>Some systems provide very robust isolation when it comes to performance. This makes it much easier for teams to share resources, as you can cap the amount of disk, CPU, memory, and network (or slot usage, if in a cluster) thatâ€™s being used. Other systems have very little support in this area. You might find yourself boxed into having to deploy many clusters to provide the necessary performance isolation that you need.<h3>Security</h3><p>Consider whether itâ€™s safe for workloads to have access to each otherâ€™s resources (data, cpu, network, filesystem, etc). This is everything from the file system, to the network, to memory. Even if access is restricted, is it might not be acceptable to run on the same physical machine.<p>There might also be hard-requirements on network isolation. <a href=https://en.wikipedia.org/wiki/Payment_Card_Industry_Data_Security_Standard>PCI DSS</a> is a common example where youâ€™re forced to segment networks, and keep infrastructure as separate as possible.<h3>Deployment</h3><p>Deployment isolation includes the kinds of things a user needs to run their workload. Do they need GPUs attached? Do they need scipy, numpy, nltk, or fortran installed? If your running in a shared cluster, are you installing this stuff on every machine? Are you going to have a subset of machines with the required resources, and use tagging or queues to force jobs on to specific nodes. Can you share these nodes with other teams if theyâ€™re not being used at a given moment? Itâ€™s important to think through each teamâ€™s requirements, as well as what the infrastructure itself can support.<p>What about package deployment? I need version 7 of some package, but some other team needs version 11. Theyâ€™re API incompatible. Are we running on the same classpath? Are the packages installed system-wide? I want Python 3, but someone else wants Python 2. <a href=https://www.docker.com/>Docker</a> is definitely helping in this area, but itâ€™s still something to consider.<h2>Scalability</h2><p>If your infrastructure isnâ€™t going to scale horizontally, you can shard vertically, but eventually youâ€™re going to bump up against hard limits. This is going to force you to split things up. On the flip side, if the infrastructure in question scales out, running a single shared deployment becomes a real possibility.<h2>Migration</h2><p>How easy is it to migrate from a single cluster to many, or vice-versa. Is it easy to shift one user off to their own system if they begin causing problems? Is it easy to shift many users into a single system if it begins to cost too much? Discussing how migration works can also help lessen the importance of a one vs. many discussion: if itâ€™s easy to migrate users, starting one way doesnâ€™t mean youâ€™re stuck that way forever if you change your mind.<h2>Automation</h2><p>If itâ€™s difficult to automate the deployment and operation of a system, the fewer the better. Deploying a separate cluster manually can be a painstaking process, and is probably going to be unacceptable. If itâ€™s going to be difficult to automate, the fewer clusters the better, usually.<p>Early on in Kafkaâ€™s development, deployment automation was quite complicated. Scripts had to execute a "clean shutdown" for a broker, and coordinate deployments so that only one or two nodes were offline at a time. Partition location also had to be taken into account to prevent partitions from going offline. This is heavy duty stuff, and needs to be fully automated. Doing this kind of work manually, or on dozens of different clusters is usually not acceptable.<h2>SLAs</h2><p>Some teams might have workloads that are mission critical. If the infrastructure they need is down, someoneâ€™s getting paged at 3 a.m. Other teams might have workloads where it is acceptable to have an hour or more of downtime. Putting these workloads on the same cluster can cause the <a href=https://en.wikipedia.org/wiki/Service-level_agreement>SLAs</a> between the two workloads to bleed together. The smaller the footprint of mission critical infrastructure, the better.<h2>Cost</h2><p>Running many smaller clusters often leads to less utilization. Since resources arenâ€™t shared, systems tend to sit idle more often. Systems like <a href=https://mesos.apache.org/>Mesos</a> and <a href=https://kubernetes.io/>Kubernetes</a> are helping in this area, as are cloud hosted systems that can be turned on and off as needed.<p>Software licensing is also something to consider. The more machines and the less utilization, the more expensive licensing can get under some structures.<h2>Conwayâ€™s law</h2><p>As unfortunate as it is, sometimes you have to think about team and organization structure. Itâ€™s common, for example, to deploy <a href=https://github.com/apache/incubator-airflow/>Airflow</a> with a 1:1 mapping to teams; one for the data science team, one for the profile team, etc. Sometimes this is for organization reasons (different ops teams, for example). Sometimes itâ€™s a reflection of different requirements for SLAs, workload, hardware, and so on.<h2>Multi-region</h2><p>Are you running your company out of multiple regions? Does the system support multi-regional deployments? Is the latency acceptable when replicating between regions? You might be forced into one deployment per-region depending on requirements.<h2>Middle ground</h2><p>In the end, the right solution is usually somewhere in the middle. Itâ€™s rare to truly have just one cluster. Usually, you at least end up with a production and non-production cluster. Then maybe you split by region or SLA, or something. Itâ€™s also rare to truly have one cluster per-user, though cloud-based solutions like RDS and EMR are making that more common. The best thing to do is think things through, have an informed discussion with everybody, and be flexible.</main>