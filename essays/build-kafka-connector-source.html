<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="date" content='2017-08-07'>
<meta name="viewport" content="width=device-width,height=device-height,initial-scale=1.0"/>
<meta property="og:type" content="website" />
<meta property="og:title" content="So, you want to build a Kafka Connector? Source edition." />
<meta property="og:description" content="I've been talking to some of the folks at Data Mountaineer about their new Cassandra CDC connector for Kafka connect, and I wanted to record some of the nuances that developers should consider when bu" />
<meta property="og:image" content="https://cnr.sh/img/og-img.png" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" integrity="sha512-HK5fgLBL+xu6dm/Ii3z4xhlSUyZgTT9tuc/hSrtw6uzJOvgRr2a9jyxxT1ely+B+xFAmJKVSTbpM/CuL7qxO8w==" crossorigin="anonymous" />
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üí¨</text></svg>"><title>So, you want to build a Kafka Connector? Source edition. | Chris Riccomini</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Lora:wght@400;700&family=Roboto+Condensed:wght@700&display=swap');
body {padding: 1.25em 0em; margin: 0em; font-family: 'Lora', serif; font-size: 14pt; line-height: 1.75;}
h1, h2, h3, h4 {font-family: 'Roboto Condensed', sans-serif; margin: 0em; line-height: 1.25;}
h1 {font-size: 30pt;}
a {color: #ff0080; text-decoration: none; font-weight: bold;}
div.home {position: fixed; font-size: 15pt; padding: .5em; line-height: 0em; background-color: #333;}
div.social {position: fixed; font-size: 15pt; padding: .5em; line-height: 0em; right: 1.5em;}
div.title {text-align: center;}
div.main {width: 32em; margin: 0em auto;}
div.intro {width: 30em; margin: 0em auto;}
div.article:first-letter {float: left; margin: .125em .25em; font-size: 4em; line-height: 1;}
div.links {width: 40em; margin: 0em auto;}
div.links table {border-spacing: 0em 0em; margin: 1em 0em;}
div.links td.link-date {text-align: right; width: 10em; padding-right: .5em; vertical-align: top;}
div.links span.link-date {display: none;}
i.social {color: #ccc;}
i.home {color: white;}
div.intro p:first-of-type {font-size: 22pt;}
div.article p:first-of-type {font-size: 18pt;}
div.article blockquote > p:first-of-type {font-size: 14pt;}
div.article img {width: 100%;}
@media screen and (min-width : 0px) and (max-width : 767px){
body {padding: 0em;}
div.main {width: auto; padding: 4em 2em;}
div.intro {width: auto; padding: 0em 3em;}
div.home {position: absolute;}
div.social {display: none;}
span.by {display: block; margin-top: .5em;}
div.links {padding-top: 1em; width: auto; padding: 4em 2em;}
div.links td.link-date {display: none;}
div.links span.link-date {display: block;}
div.links table {border-spacing: 0em 1em; margin: 0em;}
}</style>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3R91BN0RW0"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-3R91BN0RW0');
</script></head>
<body>
<div class="home">
<a href="/"><i class="home fas fa-home"></i></a>
</div>
<div class="social">
<a href="https://twitter.com/criccomini"><i class="social fab fa-twitter"></i></a>
</div><div class="main">
<div class="title">
<h1>So, you want to build a Kafka Connector? Source edition.</h1>
<span class="by">Chris Riccomini on August 7, 2017</span>
</div>
<div class="article">
<p>I‚Äôve been <a href="https://github.com/datamountaineer/stream-reactor/issues/162">talking</a> to some of the folks at <a href="https://datamountaineer.com/">Data Mountaineer</a> about their new <a href="http://docs.datamountaineer.com/en/latest/cassandra-cdc.html">Cassandra CDC connector</a> for <a href="http://docs.confluent.io/current/connect/intro.html">Kafka connect</a>, and I wanted to record some of the nuances that developers should consider when building out a new Kafka connect <a href="http://docs.confluent.io/3.3.0/connect/javadocs/index.html?org/apache/kafka/connect/source/SourceConnector.html">source connector</a>. I‚Äôm primarily focusing on source connectors where the upstream source is some kind of database.</p>
<h2 id="to-poll-or-not-to-poll">To poll or not to poll</h2>
<p>The most basic architectural decision to make is whether the source connector is going to be periodically polling the upstream system using a traditional read mechanism (running <code>SELECT * FROM...</code> every 60 seconds) or whether some replication protocol is going to be used (<a href="https://dev.mysql.com/doc/refman/5.7/en/binary-log.html">MySQL binary log</a>, <a href="https://cassandra.apache.org/doc/latest/operating/cdc.html">Cassandra CDC</a>, <a href="https://docs.mongodb.com/manual/core/replica-set-oplog/">MongoDB op_log</a>, etc.). There are some trade-offs between these two approaches, but in general, the replication-based connectors are going to be much more robust for a number of reasons:</p>
<ol type="1">
<li>They can handle <a href="https://www.quora.com/What-is-the-difference-between-soft-delete-and-hard-delete-in-SQL-Informatica-power-center-and-Informatica-cloud">hard deletes</a>.</li>
<li>You get to see all updates to a row, not just the the state of the row at each polling interval.</li>
<li>They impose fewer schema requirements than polling solutions. Usually, no modify time columns or indices are required.</li>
<li>They‚Äôre more performant, as they‚Äôre not constantly running queries on the DB.</li>
<li>They‚Äôre lower latency. You usually get updates nearly immediately when they occur, as opposed to having to wait for polling intervals.</li>
</ol>
<p>Still, there are some upsides to the polling based solutions:</p>
<ol type="1">
<li>They are usually easier to implement.</li>
<li>They don‚Äôt require that the database have exotic replication features. As long as you can read and sort data, polling should work. Not all databases even expose a replication protocol for external use.</li>
<li>They can be easier to deploy. Some databases require plugins or daemons to be running on the same machine as the database nodes in order to tap into replication or CDC features. Postgres requires that you build a <a href="http://debezium.io/docs/connectors/postgresql/#output-plugin">server-side C plugin</a> in order for connectors to tap into its feed. The Cassandra CDC implementation from Data Mountaineers requires running Kafka connect workers on the same nodes as Cassandra, itself.</li>
<li>You have to worry less about consistency issues with distributed databases. If you tap into the raw replication feeds, such as Cassandra‚Äôs CDC files, you might end up exposing duplicate rows, or raw data that downstream consumers will have to de-duplicate or parse.</li>
<li>It is easier to snapshot old data. (See below.)</li>
</ol>
<h2 id="oh-you-want-all-the-data">Oh, you want all the data?</h2>
<p>A commonly overlooked feature that‚Äôs a must-have in most production systems is the ability to snapshot (or bootstrap) data that already exists on the database. This seems obvious, but most implementations I‚Äôve come across don‚Äôt consider this at first.</p>
<p>In the case of poll-based solutions, it‚Äôs fairly straight forward to incrementally load data starting from row zero, but in the case of replication-based solutions, replication feeds do not always expose the ability to start from row zero and load all data going forward.</p>
<p>Debezium, for example, implements a <a href="https://issues.jboss.org/browse/DBZ-31">mysqldump-like</a> solution for snapshotting the initial data in MySQL. Once the snapshot is done, it flips over to the MySQL binary log. This is necessary because MySQL binary logs do not usually contain an exhaustive list of all modifications for all time; they get truncated like Kafka topics. Doing this dance between snapshot mode and replication mode can be tricky to get right.</p>
<p>Supporting this feature is helpful for end-users because otherwise you only get data going forward. For log-based tables, that might be fine, but for most primary data stores (e.g.¬†users, groups, payment methods, etc.) you generally want all the data in the upstream tables, not just new data.</p>
<p><em>Aside: Yelp! Engineering had an interesting snapshot pattern that they discuss in <a href="https://engineeringblog.yelp.com/2016/08/streaming-mysql-tables-in-real-time-to-kafka.html">Streaming MySQL tables in real-time to Kafka</a>. The pattern amounts to making a temporary copy of a table into a <a href="https://dev.mysql.com/doc/refman/5.7/en/blackhole-storage-engine.html">blackhole</a> storage engine, so they can force all data into the replication feed (binary log, in their case), rather than having two separate implementations for snapshotting versus ongoing replication.</em></p>
<h2 id="keys-please">Keys, please</h2>
<p>Another non-obvious requirement is defining the message keys that are to be persisted into Kafka such that they will work with Kafka‚Äôs <a href="https://kafka.apache.org/documentation/#compaction">log compacted</a> topics, and that they will preserve ordering.</p>
<h3 id="compaction">Compaction</h3>
<p>When replicating primary data from a database, it makes a lot of sense to have Kafka compact and remove older updates for a given record after some time. This prevents the log from growing too large, yet still gives you the ability to read all of the current data directly from Kafka, rather than having to snapshot data from the source system because it‚Äôs fallen off the edge of a Kafka‚Äôs topic‚Äôs time retention.</p>
<p>An example helps illustrate the usefulness of this feature. If I have an <em>orders</em> table with a primary key and a title field, and a given row has:</p>
<ul>
<li>INSERT orders (id, title) values (1, ‚Äúfoo‚Äù)</li>
<li>UPDATE orders (id, title) values (1, ‚Äúbar‚Äù)</li>
<li>UPDATE orders (id, title) values (1, ‚Äúbaz‚Äù)</li>
</ul>
<p>The Kafka topic will likely end up with three messages for this row, one with the value of <em>foo</em>, one with <em>bar</em>, and one with <em>baz</em>. Most users really only care about the most recent value for a given row, and if you‚Äôre using log compaction, you can have Kafka delete the older messages, such that only the <em>baz</em> message remains. In order for this to work, though, the BYTE values for the keys of all three messages must be identical, otherwise Kafka won‚Äôt be able to tell that the three messages correspond to the same row.</p>
<p>The implication that all keys for a given row must have the exact same byte value is that you can‚Äôt include any dynamic data in the key. You can‚Äôt include timestamps, mutation type (INSERT, UPDATE, DELETE), hostname, etc. You should generally keep the key as stripped down as possible, and include everything in the value payload.</p>
<p>A second, perhaps non-obvious, requirement is that the row‚Äôs primary key be included in the key payload. This is ultimately the field that is going to differentiate one row from another within log compacted topics.</p>
<h3 id="partitioning">Partitioning</h3>
<p>The second thing to pay attention to with keys is that messages for the same row must be sent to the same partition by the same producer (a connect task, in this case). This is the only way to guarantee that the mutations are seen in order by downstream consumers.</p>
<p>The requirement isn‚Äôt quite as heavy as the identical-byte one imposed by compaction since you are theoretically able to provide custom partitioners that could, for example, pay attention only to the ‚Äúid‚Äù field in the key. In practice, however, you want stuff to work out of the box with the standard Kafka partitioner, which is going to require that the keys are <a href="https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java#L69">hash and mod</a> correctly.</p>
<h2 id="modeling-deletes">Modeling deletes</h2>
<p>If your source is feeding off of a traditional database that supports hard deletes (i.e.¬†removing a row from the database), you‚Äôll have to figure out how your connector handles this. There are three common options:</p>
<ul>
<li>Send a message where the value denotes what was deleted</li>
<li>Send a message where the value is null</li>
<li>Don‚Äôt handle them</li>
</ul>
<p>Keep in mind that the key for the messages being sent to Kafka should still contain the necessary information to identify the affected row (its primary key).</p>
<p>The nice thing about the first option, where the value includes what was deleted, is that you have the necessary state to see what‚Äôs changed without querying an outside store‚Ää‚Äî‚Ääyou can see all the row data that‚Äôs been deleted. The advantage of the second approach (null value) is that it works with log compaction. Kafka will eventually compact out the record, which keeps the topic size manageable.</p>
<p>Note that these two options are not mutually exclusive. In fact, Debezium <a href="https://issues.jboss.org/browse/DBZ-45">first sends a soft delete message, and then follows up with a hard delete</a> message immediately after. This way, consumers can see what was affected, but Kafka will still compact out the deleted rows.</p>
<p>The last option, ignoring deletes, is what most polling-based solutions implement. If a row has been deleted, there‚Äôs usually no way for the connector to see it since a polling connector is just retrieving recently modified rows.</p>
<h2 id="you-get-a-schema">You get a schema!</h2>
<p>You need to think through how you map your source database‚Äôs schemas to the Kafka connect schema types. This is not as easy as it sounds. For example, what‚Äôs the <a href="https://issues.jboss.org/browse/DBZ-228">right way to handle an UNSIGNED BIGINT</a>? What if binary values are <a href="https://issues.jboss.org/browse/DBZ-254">not padded properly</a>? Debezium came across a number of these issues both in their Postgres and MySQL implementations. You also need to pay special attention to logical types. There are <a href="https://avro.apache.org/docs/1.8.0/spec.html#Logical+Types">Avro logical types</a>, <a href="https://issues.apache.org/jira/browse/KAFKA-2476">Kafka connect logical types</a>, and even <a href="http://debezium.io/docs/connectors/mysql/#data-types">connector-level logical types</a>. Understanding what logical types are and how they‚Äôre used is a must.</p>
<h3 id="schema-registry">Schema registry</h3>
<p>Special attention also needs to be paid to how you mutate your messages‚Äô schemas over time. You have to assume that some of your users will also be using Avro and Confluent‚Äôs schema registry. Some of them will be running the registry with <a href="http://docs.confluent.io/current/schema-registry/docs/api.html#id1">compatibility checks enabled</a>. It‚Äôs imperative that fields not be added and removed from the messages for a topic in an incompatible way to make your users‚Äô life easier. Some of this burden also falls on the user, as they‚Äôd be tying the mutation of their upstream DB and table schemas to their downstream Kafka topics.</p>
<h2 id="so-i-downloaded-this-thing">So I downloaded this thing‚Ä¶</h2>
<p>Lastly, a lot of help needs to be given on how to operate the connector. This goes beyond just how to configure and deploy the connector itself. Often times, the proper way to run these connectors will require deploying a replica cluster or special set of nodes for the upstream database.</p>
<p>At WePay we run a <a href="https://wecode.wepay.com/posts/streaming-databases-in-realtime-with-mysql-debezium-kafka#architecture">separate MySQL replica cluster</a> for Debezium to feed off of. This has a number of benefits including isolation when snapshots occur, custom configuration for the nodes (e.g.¬†longer binary log retention), and so on. Providing guidance in this area will make your users‚Äô lives a lot easier.</p>
<p>Discussing patterns when running in a cloud environment can also be helpful.</p>
<h2 id="thats-all-folks">That‚Äôs all folks</h2>
<p>This is an off-the-top-of-my-head list of things to pay attention to. There are probably more, and I welcome feedback. In any case, hopefully this doesn‚Äôt scare you too much, and leads to more Kafka connectors.</p>
</div>
<form style="background-color: #eee; padding:1em; text-align:center;" action="https://tinyletter.com/criccomini" method="post" target="popupwindow" onsubmit="window.open('https://tinyletter.com/criccomini', 'popupwindow', 'scrollbars=yes,width=800,height=600');return true">
<h1><label for="tlemail">Never Miss a Post</label></h1>
<span class="by">Subscribe to my newsletter!</span>
<p style="margin-bottom: .35em;">
<input type="text" style="width:140px" name="email" id="tlemail" placeholder="your@email.com" />
<input type="hidden" value="1" name="embed"/><input type="submit" value="Subscribe" />
</p>
</form></div>
</body>
</html>