<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="date" content='2017-08-14'>
<title>One big cluster, or many small ones? | Chris Riccomini</title>
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css" integrity="sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz" crossorigin="anonymous"/>
<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üí¨</text></svg>">
<style>
@import url('https://fonts.googleapis.com/css2?family=Lora:wght@400;700&family=Roboto+Condensed:wght@700&display=swap');
body {margin: 0px;}
h1 {font-family: 'Roboto Condensed', sans-serif; font-size: 30pt; margin: 0px;}
h2 {font-family: 'Roboto Condensed', sans-serif; font-size: 20pt; margin: 0px; line-height: 1;}
img {width: 100%;}
a {color: #ff0080; text-decoration: none; font-weight: bold;}
div.home {position: fixed; font-size: 15pt; padding: 10px; background-color: #333;}
div.social {position: fixed; right: 25px; padding: 10px; font-size: 15pt;;}
div.title {text-align: center;}
/* TODO responsive width */
div.main {width: 600px; margin: 25px auto;}
span.by {font-family: 'Lora', serif; font-size: 14pt; line-height: 1.75;}
div.article {line-height: 1.75; font-family: 'Lora', serif; font-size: 14pt;}
div.article:first-letter {float: left; margin: .125em .25em; font-size: 4em; line-height: 1;}
i.social {margin-left: 10px; color: #ccc;}
i.home {color: white;}
</style>
</head>
<body>
<div class="home">
<a href="/"><i class="home fas fa-home"></i></a>
</div>
<div class="social">
<a href="https://twitter.com/criccomini"><i class="social fab fa-twitter"></i></a>
</div>
<div class="main">
<div class="title">
<h1>One big cluster, or many small ones?</h1>
<span class="by">Chris Riccomini on August 14, 2017</span>
</div>
<div class="article">
<p>I bumped into something recently that seems to recur at every company I work for. Should we run one big cluster, or many smaller ones? The discussion is usually triggered when you have more than one team that wants to use the infrastructure in question.</p>
<p>The question is, should an organization run a single instance of the infrastructure, or many physically separate instances (sometimes called federation, though it‚Äôs an overloaded term). This question holds true for both clustered solutions (<a href="https://kafka.apache.org/">Kafka</a>, <a href="https://hadoop.apache.org/">Hadoop</a>, <a href="https://cassandra.apache.org/">Cassandra</a>, etc.) and more traditional single node infrastructure (<a href="https://www.mysql.com/">MySQL</a>, <a href="https://www.postgresql.org/">Postgres</a>, etc.).</p>
<p>My most recent foray into the discussion centered around MySQL deployments, but I‚Äôve seen the same discussion play out for Hadoop, Kafka, and some other internal database stuff when I was at LinkedIn.</p>
<p>People sometimes feel pretty strongly about the right approach. In the end, as usual, it‚Äôs a trade off, and the right answer is dependent on a lot of variables. That said, you should consider at least the following when thinking through this problem:</p>
<ul>
<li>Isolation</li>
<li>Security</li>
<li>Scalability</li>
<li>Migration</li>
<li>Automation</li>
<li>Deployment heterogeneity</li>
<li>SLAs</li>
<li>Workload</li>
<li>Cost</li>
<li>Conway‚Äôs law</li>
<li>Multi-region</li>
</ul>
<p>Let‚Äôs take a look at these.</p>
<h2 id="isolation">Isolation</h2>
<p>How comfortable are you with one user affecting another user on the system? Isolation can include data isolation, package isolation, query isolation, CPU isolation, network isolation, and a lot more. I‚Äôm going to split isolation into several categories: performance, security, and deployment.</p>
<h3 id="performance">Performance</h3>
<p>Some systems provide very robust isolation when it comes to performance. This makes it much easier for teams to share resources, as you can cap the amount of disk, CPU, memory, and network (or slot usage, if in a cluster) that‚Äôs being used. Other systems have very little support in this area. You might find yourself boxed into having to deploy many clusters to provide the necessary performance isolation that you need.</p>
<h3 id="security">Security</h3>
<p>Consider whether it‚Äôs safe for workloads to have access to each other‚Äôs resources (data, cpu, network, filesystem, etc). This is everything from the file system, to the network, to memory. Even if access is restricted, is it might not be acceptable to run on the same physical machine.</p>
<p>There might also be hard-requirements on network isolation. <a href="https://en.wikipedia.org/wiki/Payment_Card_Industry_Data_Security_Standard">PCI DSS</a> is a common example where you‚Äôre forced to segment networks, and keep infrastructure as separate as possible.</p>
<h3 id="deployment">Deployment</h3>
<p>Deployment isolation includes the kinds of things a user needs to run their workload. Do they need GPUs attached? Do they need scipy, numpy, nltk, or fortran installed? If your running in a shared cluster, are you installing this stuff on every machine? Are you going to have a subset of machines with the required resources, and use tagging or queues to force jobs on to specific nodes. Can you share these nodes with other teams if they‚Äôre not being used at a given moment? It‚Äôs important to think through each team‚Äôs requirements, as well as what the infrastructure itself can support.</p>
<p>What about package deployment? I need version 7 of some package, but some other team needs version 11. They‚Äôre API incompatible. Are we running on the same classpath? Are the packages installed system-wide? I want Python 3, but someone else wants Python 2. <a href="https://www.docker.com/">Docker</a> is definitely helping in this area, but it‚Äôs still something to consider.</p>
<h2 id="scalability">Scalability</h2>
<p>If your infrastructure isn‚Äôt going to scale horizontally, you can shard vertically, but eventually you‚Äôre going to bump up against hard limits. This is going to force you to split things up. On the flip side, if the infrastructure in question scales out, running a single shared deployment becomes a real possibility.</p>
<h2 id="migration">Migration</h2>
<p>How easy is it to migrate from a single cluster to many, or vice-versa. Is it easy to shift one user off to their own system if they begin causing problems? Is it easy to shift many users into a single system if it begins to cost too much? Discussing how migration works can also help lessen the importance of a one vs.¬†many discussion: if it‚Äôs easy to migrate users, starting one way doesn‚Äôt mean you‚Äôre stuck that way forever if you change your mind.</p>
<h2 id="automation">Automation</h2>
<p>If it‚Äôs difficult to automate the deployment and operation of a system, the fewer the better. Deploying a separate cluster manually can be a painstaking process, and is probably going to be unacceptable. If it‚Äôs going to be difficult to automate, the fewer clusters the better, usually.</p>
<p>Early on in Kafka‚Äôs development, deployment automation was quite complicated. Scripts had to execute a ‚Äúclean shutdown‚Äù for a broker, and coordinate deployments so that only one or two nodes were offline at a time. Partition location also had to be taken into account to prevent partitions from going offline. This is heavy duty stuff, and needs to be fully automated. Doing this kind of work manually, or on dozens of different clusters is usually not acceptable.</p>
<h2 id="slas">SLAs</h2>
<p>Some teams might have workloads that are mission critical. If the infrastructure they need is down, someone‚Äôs getting paged at 3 a.m. Other teams might have workloads where it is acceptable to have an hour or more of downtime. Putting these workloads on the same cluster can cause the <a href="https://en.wikipedia.org/wiki/Service-level_agreement">SLAs</a> between the two workloads to bleed together. The smaller the footprint of mission critical infrastructure, the better.</p>
<h2 id="cost">Cost</h2>
<p>Running many smaller clusters often leads to less utilization. Since resources aren‚Äôt shared, systems tend to sit idle more often. Systems like <a href="https://mesos.apache.org/">Mesos</a> and <a href="https://kubernetes.io/">Kubernetes</a> are helping in this area, as are cloud hosted systems that can be turned on and off as needed.</p>
<p>Software licensing is also something to consider. The more machines and the less utilization, the more expensive licensing can get under some structures.</p>
<h2 id="conways-law">Conway‚Äôs law</h2>
<p>As unfortunate as it is, sometimes you have to think about team and organization structure. It‚Äôs common, for example, to deploy <a href="https://github.com/apache/incubator-airflow/">Airflow</a> with a 1:1 mapping to teams; one for the data science team, one for the profile team, etc. Sometimes this is for organization reasons (different ops teams, for example). Sometimes it‚Äôs a reflection of different requirements for SLAs, workload, hardware, and so on.</p>
<h2 id="multi-region">Multi-region</h2>
<p>Are you running your company out of multiple regions? Does the system support multi-regional deployments? Is the latency acceptable when replicating between regions? You might be forced into one deployment per-region depending on requirements.</p>
<h2 id="middle-ground">Middle ground</h2>
<p>In the end, the right solution is usually somewhere in the middle. It‚Äôs rare to truly have just one cluster. Usually, you at least end up with a production and non-production cluster. Then maybe you split by region or SLA, or something. It‚Äôs also rare to truly have one cluster per-user, though cloud-based solutions like RDS and EMR are making that more common. The best thing to do is think things through, have an informed discussion with everybody, and be flexible.</p>
</div>
</div>
</body>
</html>