<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="date" content='2019-07-29'>
<meta name="viewport" content="width=device-width,height=device-height,initial-scale=1.0"/>
<meta property="og:type" content="website" />
<meta property="og:title" content="The Future of Data Engineering" />
<meta property="og:description" content="I have been thinking lately
about where we‚Äôve come in data engineering over the past few years, and
about what the future holds for work in this area. Most of this thought
has been framed in the conte" />
<meta property="og:image" content="https://cnr.sh/img/og-img.png" /><link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üí¨</text></svg>">
<link rel="alternate" type="application/rss+xml" href="https://cnr.sh/rss.xml" title="Chris Riccomini's RSS Feed"><title>The
Future of Data Engineering | Chris Riccomini</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Lora:wght@400;700&family=Roboto+Condensed:wght@700&display=swap');
body {padding: 1.25rem 0em; margin: 0em; font-family: 'Lora', serif; font-size: 14pt; line-height: 1.75;}
h1, h2, h3, h4 {font-family: 'Roboto Condensed', sans-serif; margin: 0em; line-height: 1.25;}
h1 {font-size: 30pt;}
a {color: #ff0080; text-decoration: none; font-weight: bold;}
pre > code {overflow-x: auto; width: 100%; display: inline-block;}
div.home {position: fixed; font-size: 15pt; padding: .5em; line-height: 0em; background-color: #333;}
div.social {position: fixed; font-size: 15pt; padding: .5em; line-height: 0em; right: 1.5em;}
div.title {text-align: center;}
div.main {width: 32em; margin: 0em auto;}
div.intro {width: 30em; margin: 0em auto;}
div.article:first-letter {float: left; margin: .125em .25em; font-size: 4em; line-height: 1;}
div.links {width: 40em; margin: 0em auto;}
div.links table {border-spacing: 0em 0em; margin: 1em 0em;}
div.links td.link-date {text-align: right; width: 10em; padding-right: .5em; vertical-align: top;}
div.links span.link-date {display: none;}
i.social {color: #ccc;}
i.home {color: white;}
div.intro p:first-of-type {font-size: 22pt;}
div.article p:first-of-type {font-size: 18pt;}
div.article blockquote > p:first-of-type {font-size: 14pt;}
div.article img {width: 100%;}
div.promo {margin: 0rem auto 1.25rem auto; width: 50rem; font-family: helvetica; text-align: center; top: 0px; left: 0px; background-color: black; color: white; font-size: .925rem; letter-spacing: .075rem; padding: .5em;}
@media screen and (min-width : 0px) and (max-width : 767px){
body {padding: 0em;}
div.main {width: auto; padding: 4em 2em;}
div.intro {width: auto; padding: 0em 3em;}
div.home {position: absolute;}
div.social {display: none;}
span.by {display: block; margin-top: .5em;}
div.links {padding-top: 1em; width: auto; padding: 4em 2em;}
div.links td.link-date {display: none;}
div.links span.link-date {display: block;}
div.links table {border-spacing: 0em 1em; margin: 0em;}
div.promo {width: auto; padding: 1rem 4rem; }
}</style>
<script src="https://kit.fontawesome.com/72c21ff1f4.js" crossorigin="anonymous"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3R91BN0RW0"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-3R91BN0RW0');
</script></head>
<body>
<div class="home">
<a href="/"><i class="home fas fa-home"></i></a>
</div>
<div class="social">
<a href="https://twitter.com/criccomini"><i class="social fab fa-twitter"></i></a>
</div><div class="promo">
<a href="https://highwire.ai" style="color: #fff; font-weight: normal;">
My latest project, <span style="font-weight: bold; text-decoration: underline;">Highwire</span>, is up! Connect your brokerage, set asset targets, and get portfolio alerts.
</a>
</div><div class="main">
<div class="title">
<h1>The Future of Data Engineering</h1>
<span class="by">Chris Riccomini on July 29, 2019</span>
</div>
<div class="article">
<p>I have been thinking lately about where we‚Äôve come in data
engineering over the past few years, and about what the future holds for
work in this area. Most of this thought has been framed in the context
of what some of our teams are doing at WePay, but I believe the
framework below applies more broadly, and is worth sharing.</p>
<p><em>I presented an expanded version of this blog post for QCon SF
2019. The video of the talk is available <a
href="https://www.infoq.com/presentations/data-engineering-pipelines-warehouses">here</a>.</em></p>
<p>Data engineering‚Äôs job is to help an organization move and process
data. This generally requires two different systems, broadly speaking: a
data pipeline, and a data warehouse. The data pipeline is responsible
for moving the data, and the data warehouse is responsible for
processing it. I acknowledge that this is a bit overly simplistic. You
can do processing in the pipeline itself by doing transformations
between extraction and loading with batch and stream processing. The
‚Äúdata warehouse‚Äù now includes many storage and processing systems
(Flink, Spark, Presto, Hive, BigQuery, Redshift, etc), as well as
auxiliary systems such as data catalogs, job schedulers, and so on.
Still, I believe the paradigm holds.</p>
<p>The industry is working through changes in how these systems are
built and managed. There are four areas, in particular, where I expect
to see shifts over the next few years.</p>
<ul>
<li>Timeliness: From batch to realtime</li>
<li>Connectivity: From one:one bespoke integrations to many:many</li>
<li>Centralization: From centrally managed to self-serve tooling</li>
<li>Automation: From manually managed to automated tooling</li>
</ul>
<h2 id="from-batch-to-realtime">From batch to realtime</h2>
<p>Both data pipelines and data processing have been batch-based in the
past. Data was transported between systems in batch ETL snapshots, and
data was processed in a periodic cadence, which was managed by a job
scheduler (Airflow, Oozie, Azkaban, Luigi).</p>
<p>We are now shifting to both realtime data pipelines and realtime data
processing systems. Change data capture systems such as Debezium, as
well as the robust connector ecosystem in Kafka have made realtime data
pipelines possible. Stream processing has gone through a renaissance
over the last five years, and there are now more realtime data
processing systems than I can keep track of. The combination of these
factors means that the ingress (extraction), processing
(transformation), and egress (loading) can all happen in realtime.</p>
<p>The trade off right now seems to be in cost and complexity. If a
company is getting a data warehouse off the ground, and needs an
immediate 4-6 week impact, it‚Äôs a heavy load to set up a realtime
pipeline. Batch is still much easier to get up and running. I expect
that this will change over the next few years as tooling in the realtime
area continues to mature, and cloud hosting continues to grow.</p>
<h2 id="connectivity">Connectivity</h2>
<p>Connecting upstream data sources to the data warehouse used to mean
adding an entirely new bespoke integration for each one-to-one
connection between systems. <a href="https://twitter.com/jaykreps">Jay
Kreps</a>‚Äô seminal post, <a
href="https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">The
Log: What every software engineer should know about real-time data‚Äôs
unifying abstraction</a>, illustrates this well:</p>
<p><img
src="/img/essays/2019-07-29-future-data-engineering/datapipeline_complex.png"
title="Complex data pipeline" alt="Complex data pipeline" /></p>
<p>The post also goes into detail about the future of data integration.
It was written in 2013, and a lot of what is predicted and proposed is
just coming to fruition today. The launch of <a
href="https://www.confluent.io/">Confluent</a>, <a
href="https://docs.confluent.io/current/connect/index.html">Kafka
Connect</a>, and the connector ecosystem now mean that there are many
viable connectors to attach to an existing Kafka data pipeline.</p>
<p>I suspect we‚Äôll begin to see this architectural approach take hold.
Detaching ingress from egress, and both from transform means that data
pipelines can begin to take advantage of <a
href="https://en.wikipedia.org/wiki/Metcalfe%27s_law">Metcalfe‚Äôs
law</a>, where adding a new system to the pipeline will be both cheaper
and more valuable than it was before.</p>
<p>I‚Äôm aware that the paragraphs above are quite Kafka-centric, but I
want to note that this approach need not be realtime (though I believe
it will be). In <a
href="https://github.com/apache/airflow/">Airflow</a>, for example, you
currently see a GoogleCloudStorageHook, a BigQueryHook, and then a
one-to-one operator: GoogleCloudStorageToBigQueryOperator. Untangling
ingest from egress fully, so that a staging area in a common format
exists would go a long way in improving even batch-based ETL.</p>
<p>This pattern will also allow us to embrace more fine-grained data
systems. Once it‚Äôs cheap to attach new systems to the pipeline, the
value of adding a new specialized system to the ecosystem will outweigh
its cost. As a result, I expect to see more usage of niche data
processing systems such as graph databases, realtime OLAP systems,
search indices, and the like. The pattern will also allow for more
experimentation, since attaching a new system to the pipeline and then
discovering it doesn‚Äôt meet your need is much cheaper than it used to
be. (This is a theme that I‚Äôve already covered in a post from earlier
this year, <a href="https://riccomini.name/kafka-escape-hatch">Kafka is
your escape hatch</a>.)</p>
<p>The cloud also adds an interesting wrinkle to the connectivity story.
The dream of having your data integration story amount to a series of
checkboxes in an AWS console is not yet realized, and I don‚Äôt expect
that we‚Äôll see a full integration amongst all systems in each cloud
provider any time soon. The idea of full integration between cloud
providers in a point-and-click UI seems even farther off. Lastly,
integrating cloud and non-cloud (or even third-party hosted) systems
will still require work.</p>
<p>For these reasons, I think cloud-based third-party solutions such as
<a href="https://www.stitchdata.com/">Stitch</a> will continue to be
valuable. This also means that the realtime Kafka architecture that I
describe above will be the most mature solution if you can afford to
build and operate it.</p>
<h2 id="automation-decentralization">Automation &amp;
decentralization</h2>
<p>The last two items in my list, automation and centralization, really
go hand-in hand. Most organizations have a single data engineering
and/or data warehousing team that manages the data pipeline and data
warehouse. When a request comes into these teams, they need to evaluate
the request across two criteria:</p>
<ul>
<li>What can we do (technical)</li>
<li>What may we do (policy)</li>
</ul>
<p>In my experience, a centralized team will usually have some
automation, but it will be mostly focused on technical automation. This
is natural, as it‚Äôs more in an engineering team‚Äôs wheel house. This kind
of work usually means replacing operational <a
href="https://landing.google.com/sre/sre-book/chapters/eliminating-toil/">toil</a>
with automation; activities like adding a new connector, setting up
monitoring or data quality checks, creating a new database or table,
granting permission, and so on.</p>
<p>There is, however, a second type of toil that I believe data
engineering has not yet automated: policy toil. This kind of drudgery
involves making decision about who can have access to what data, how
long data should be persisted, what kind of sensitive data is allowed to
be in which data systems, and in which geographies data may reside. Data
engineering is usually not the team that ultimately decides the answers
to these questions, but they often must act as a liason or driver when
finding the answers. This usually means navigating requests through
other parts of the organization such as security, compliance, and
legal.</p>
<p>This kind of work is already important because of regulation such as
<a href="https://eugdpr.org/">GDPR</a> and <a
href="https://www.caprivacy.org/">CCPA</a>. If I add government
regulation to the ongoing expansion of tech beyond traditional software
companies, and into areas such as health and finance (see <a
href="https://a16z.com/2011/08/20/why-software-is-eating-the-world/">Why
Software Is Eating the World</a>), it is inevitable that the importance
of automating policy toil will only grow.</p>
<p>Policy automation will require focus in areas that are usually
neglected in less mature data ecosystems. Tooling such as Lyft‚Äôs <a
href="https://eng.lyft.com/amundsen-lyfts-data-discovery-metadata-engine-62d27254fbb9">Amundsen</a>,
<a href="https://ranger.apache.org/">Apache Ranger</a>, and Google‚Äôs <a
href="https://cloud.google.com/data-catalog/">Data Catalog</a> will need
to be adopted. Policy enforcement such as audits, <a
href="https://en.wikipedia.org/wiki/DLP">DLP</a>, sensitive data
detection, retention enforcement, and access control management will all
need to be fully automated.</p>
<p>As automation matures in both the technical and policy areas, the
next logical question will be: why do we need a single team to manage
this? If tools are enforcing policy guidelines, and automating the data
pipeline, why not empower teams around the organization to directly
manage their data pipelines and data warehouses?</p>
<p>On the data pipeline front, decentralization will mean that any team
may decide to plug into the existing data pipeline, provided that they
conform to (automatically enforced) technical and policy guidelines. In
the data warehouse, teams will be able to create databases, datasets,
data marts, and data lakes as their needs arise (some of this already
exists today). This will cause a lot of complexity, confusion, and
duplication, which is why tooling (such as those listed above) is such
an important prerequisite to decentralization.</p>
<p>The topic of decentralization is something that‚Äôs covered in greater
detail in <a href="https://twitter.com/zhamakd">Zhamak Dehghani</a>‚Äôs
post, <a
href="https://martinfowler.com/articles/data-monolith-to-mesh.html">How
to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh</a>,
and I invite you to read it. Due to the <a
href="https://techbeacon.com/app-dev-testing/forget-monoliths-vs-microservices-cognitive-load-what-matters">cognitive
load</a> required to manage a more complex data ecosystem, I suspect the
only scalable and efficient way forward will be through automation and
decentralization. This will, in some ways, look like the <a
href="https://en.wikipedia.org/wiki/CI/CD">CI/CD</a> and
monolith-to-micro-service migration we‚Äôve seen play out over the past
decade in the application layer.</p>
<h2 id="conclusion">Conclusion</h2>
<p>All of this leaves me optimistic. There‚Äôs a lot left to do, and with
that a lot of opportunity. I expect data engineering to grow in
organizations to meet these demands. Open source communities will
continue to expand, and new projects and startups will be built. All of
this should lead to massive efficiency gains for organizations as a
whole, and hopefully more rigorous data management practices.</p>
</div>
<form style="background-color: #eee; padding:1em; text-align:center;" action="https://tinyletter.com/criccomini" method="post" target="popupwindow" onsubmit="window.open('https://tinyletter.com/criccomini', 'popupwindow', 'scrollbars=yes,width=800,height=600');return true">
<h1><label for="tlemail">Never Miss a Post</label></h1>
<span class="by">Subscribe to my newsletter!</span>
<p style="margin-bottom: .35em;">
<input type="text" style="width:140px" name="email" id="tlemail" placeholder="your@email.com" />
<input type="hidden" value="1" name="embed"/><input type="submit" value="Subscribe" />
</p>
</form></div>
</body>
</html>