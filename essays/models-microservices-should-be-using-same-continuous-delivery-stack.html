<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="date" content='2018-10-29'>
<meta name="viewport" content="width=device-width,height=device-height,initial-scale=1.0"/>
<meta property="og:type" content="website" />
<meta property="og:title" content="Models and microservices should be
running on the same continuous delivery stack" />
<meta property="og:description" content="I've been interested in data
science platforms for a long time. My fascination began when I was at
LinkedIn, and helped build out the first model building and deployment
system for People You May Know" />
<meta property="og:image" content="https://cnr.sh/img/og-img.png" /><link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’¬</text></svg>">
<link rel="alternate" type="application/rss+xml" href="https://cnr.sh/rss.xml" title="Chris Riccomini's RSS Feed"><title>Models
and microservices should be running on the same continuous delivery
stack | Chris Riccomini</title>
<style>
@import url('https://fonts.googleapis.com/css2?family=Lora:wght@400;700&family=Roboto+Condensed:wght@700&display=swap');
body {padding: 1.25rem 0em; margin: 0em; font-family: 'Lora', serif; font-size: 14pt; line-height: 1.75;}
h1, h2, h3, h4 {font-family: 'Roboto Condensed', sans-serif; margin: 0em; line-height: 1.25;}
h1 {font-size: 30pt;}
a {color: #ff0080; text-decoration: none; font-weight: bold;}
pre > code {overflow-x: auto; width: 100%; display: inline-block;}
div.home {position: fixed; font-size: 15pt; padding: .5em; line-height: 0em; background-color: #333;}
div.social {position: fixed; font-size: 15pt; padding: .5em; line-height: 0em; right: 1.5em;}
div.title {text-align: center;}
div.main {width: 32em; margin: 0em auto;}
div.intro {width: 30em; margin: 0em auto;}
div.article:first-letter {float: left; margin: .125em .25em; font-size: 4em; line-height: 1;}
div.links {width: 40em; margin: 0em auto;}
div.links table {border-spacing: 0em 0em; margin: 1em 0em;}
div.links td.link-date {text-align: right; width: 10em; padding-right: .5em; vertical-align: top;}
div.links span.link-date {display: none;}
i.social {color: #ccc;}
i.home {color: white;}
div.intro p:first-of-type {font-size: 22pt;}
div.article p:first-of-type {font-size: 18pt;}
div.article blockquote > p:first-of-type {font-size: 14pt;}
div.article img {width: 100%;}
div.promo {margin: 0rem auto 1.25rem auto; width: 50rem; font-family: helvetica; text-align: center; top: 0px; left: 0px; background-color: black; color: white; font-size: .925rem; letter-spacing: .075rem; padding: .5em;}
@media screen and (min-width : 0px) and (max-width : 767px){
body {padding: 0em;}
div.main {width: auto; padding: 4em 2em;}
div.intro {width: auto; padding: 0em 3em;}
div.home {position: absolute;}
div.social {display: none;}
span.by {display: block; margin-top: .5em;}
div.links {padding-top: 1em; width: auto; padding: 4em 2em;}
div.links td.link-date {display: none;}
div.links span.link-date {display: block;}
div.links table {border-spacing: 0em 1em; margin: 0em;}
div.promo {width: auto; padding: 1rem 4rem; }
}</style>
<script defer src="https://kit.fontawesome.com/72c21ff1f4.js" crossorigin="anonymous"></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3R91BN0RW0"></script>
<script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-3R91BN0RW0');
</script></head>
<body>
<div class="home">
<a href="/"><i class="home fas fa-home"></i></a>
</div>
<div class="social">
<a href="https://twitter.com/criccomini"><i class="social fab fa-twitter"></i></a>
</div><div class="promo">
<a href="https://highwire.ai" style="color: #fff; font-weight: normal;">
Join my latest project! <span style="font-weight: bold; text-decoration: underline;">Highwire</span> alerts you when it's time to rebalance your investment portfolio.
</a>
</div><div class="main">
<div class="title">
<h1>Models and microservices should be running on the same continuous
delivery stack</h1>
<span class="by">Chris Riccomini on October 29, 2018</span>
</div>
<div class="article">
<p>Iâ€™ve been interested in data science platforms for a long time. My
fascination began when I was at LinkedIn, and helped build out the first
model building and deployment system for <a
href="https://engineering.linkedin.com/teams/data/projects/pymk">People
You May Know</a>. At the time, we did feature engineering and training
on <a href="https://hadoop.apache.org/">Hadoop</a>, job scheduling on <a
href="https://azkaban.github.io/">Azkaban</a>, and model deployment on
<a
href="https://www.project-voldemort.com/voldemort/">Voldemort</a>.</p>
<p>Things have changed a lot since then. Two posts that have caught my
attention lately are the machine learning platforms from Airbnb (<a
href="https://medium.com/airbnb-engineering/using-machine-learning-to-predict-value-of-homes-on-airbnb-9272d3d4739d">here</a>)
and Uber (<a href="https://eng.uber.com/michelangelo/">here</a>). Airbnb
has also been talking about <a
href="https://www.slideshare.net/databricks/bighead-airbnbs-endtoend-machine-learning-platform-with-krishna-puttaswamy-and-andrew-hoh">Bighead</a>,
their <a
href="https://cdn.oreillystatic.com/en/assets/1/event/278/Bighead_%20Airbnb_s%20end-to-end%20machine%20learning%20platform%20Presentation.pdf">end-to-end
ML system</a>. All of these posts do a great job breaking down all of
the different steps involved in machine learning, and how each company
built a platform to help developers with the process.</p>
<p>Something that caught my eye, though, is that the deployment pipeline
for their models is custom built. Both Airbnb and Uber have a fairly
standard <a
href="https://medium.com/airbnb-engineering/testing-at-airbnb-199f68a0a40d">continuous
deployment process</a> for <a
href="https://eng.uber.com/micro-deploy/">building and deploying</a> web
services. Yet, either deliberately or by accident, they chose to build a
separate system for their model deployment pipeline.</p>
<p>This is an interesting peculiarity. I canâ€™t recall ever seeing a
unified model and service deployment stack. Yet, in the abstract, the
deployment needs for models and services are strikingly similar.</p>
<h2 id="continuous-deployment-for-models">Continuous deployment for
models</h2>
<p>Letâ€™s start by reviewing the continuous delivery process for a
microservice.</p>
<ul>
<li>Version control</li>
<li>Build</li>
<li>Unit test</li>
<li>Deploy to staging</li>
<li>Test in staging</li>
<li>Deploy to production</li>
<li>Measure and validate</li>
</ul>
<p>This looks very similar to what youâ€™re doing with your models.</p>
<h3 id="version-control">Version control</h3>
<p>Models need to live somewhere when theyâ€™re being developed. You need
version control. You need to see who made what changes. Sounds a lot
like Git.</p>
<h3 id="build">Build</h3>
<p>A model needs to be built before it can be deployed. The process of
building a model is definitely different from building a microservice.
Microservices are built when code changes. Models are built when either
data changes or when code changes. In both cases, though, there is a
trigger (new data arrival, or a code merge) that causes a model to be
retrained or a microservice to be rebuilt.</p>
<p>Microservices are compiled. Models are trained. The training often
involves a much more involved sequence that usually requires some kind
of orchestration (like Airbnbâ€™s <a
href="https://github.com/apache/incubator-airflow">Airflow</a>).</p>
<p>Still, in the abstract, there is an event that triggers a build, and
then a sequence of steps that lead to an artifact. During the build,
models, like microservices, need to be assigned a version. Perhaps <a
href="https://semver.org/">semantic versioning</a> isnâ€™t quite the best
fit, but a versioned artifact is required nonetheless. This makes it
easier to track changes, roll forward, roll back, maintain history, etc.
<a
href="https://www.slideshare.net/databricks/bighead-airbnbs-endtoend-machine-learning-platform-with-krishna-puttaswamy-and-andrew-hoh">This
Bighead slide</a> calls out the challenge of model management
explicitly. Versioning (and version control), and things like release
notes, are how we handle this on the microservice side of the world.</p>
<p>Both microservices and models need to be deployed to production.
Theyâ€™re usually both built (or trained) in a non-production environment.
For models, this is usually a modeling (or data warehousing)
environment. For microservices, secure build infrastructure is used. The
artifacts then get published into a repository. Things get even more
interesting if you deploy microservices through Docker. You can layer
your data on top of a serviceâ€™s base image to quickly update and deploy
model changes. <a
href="https://hackernoon.com/docker-data-containers-cb250048d162">Data
containers</a> also look interesting.</p>
<h3 id="unit-test">Unit test</h3>
<p>After a build is complete, it must be tested. For models, this
usually means validating that the model is performing adequately against
holdout data. Microservices need to pass their unit tests.</p>
<h3 id="deploy-to-staging">Deploy toÂ staging</h3>
<p>Though itâ€™s not common practice, it makes sense to deploy your model
to your staging environment before it goes to production. This will
exercise the model deployment machinery, and make sure that your staging
environment continues to look like production.</p>
<h3 id="test-in-staging">Test inÂ staging</h3>
<p>Any integration or user acceptance tests that need to be run can
validate that changes in model behavior didnâ€™t break anything
obvious.</p>
<h3 id="deploy-to-production">Deploy to production</h3>
<p>After tests pass in staging, itâ€™s time to deploy to production. On
the microservice side of the fence, this usually involves either a <a
href="https://martinfowler.com/bliki/BlueGreenDeployment.html">blue/green
deployment</a> or a <a
href="https://martinfowler.com/bliki/CanaryRelease.html">canary
release</a>. Most model deployments follow a similar pattern. You deploy
the new model, shift traffic over, then remove the old model.</p>
<h3 id="measure-and-validate">Measure andÂ validate</h3>
<p>Measuring and validating microservices usually involves looking at
metric and log information. Are there any anomalies in the logs? Higher
WARNs then usual? More exceptions? Has latency spiked, are there
fewer/more threads being used, etc?</p>
<p>Again, the same pattern holds for models. The prediction success rate
of the model can be measured, and if it dips beyond a threshold, it can
be rolled back.</p>
<h2 id="what-happened">What happened?</h2>
<p>Iâ€™m not entirely sure why we ended up here.Â Two potential
explanations are:</p>
<ul>
<li>The jump from data science to data engineering to DevOps makes it
difficult for teams building data infrastructure to recognize the
commonalities with the microservice environment at their organization
(c.f. <a href="https://en.wikipedia.org/wiki/Conway&#39;s_law">Conwayâ€™s
law</a>).</li>
<li>Microservice tooling isnâ€™t flexible enough. For example, CI systems
tend to model build steps as linear or parallelizable. Model building
usually involves much more complex DAGs where tasks have dependencies on
one another.</li>
</ul>
<h2 id="same-process-or-same-stack">Same process or same stack</h2>
<p>Just because the process for releasing a model can be mapped to a
continuous deployment lifecycle doesnâ€™t necessarily mean that the same
stack should be used for both pipelines. Airflow is a decent
orchestrator for model building, but I donâ€™t think Iâ€™d want to use it to
build Java microservices. Likewise, CircleCI probably isnâ€™t what you
want to use to build a model.</p>
<p>The logical place to start is probably the deployment manager itself.
The system that models the continuous deployment lifecycle. It should be
possible to use systems like <a
href="https://www.spinnaker.io/">Spinnaker</a> to track the deployment
of a model through various environments (testing, staging, production,
etc).</p>
<p>Artifact repositories and version control are other places that seem
to be likely targets for integration. If youâ€™re building models as
pickled objects, tarballs, or Docker images, sticking them in a standard
artifact repository is quite doable. Storing the models in version
control is also obvious.</p>
<p>Measurement and validation also seem to be likely candidates. Placing
model performance in your operational charts and graphs is something
that should be done anyway, and adding alerting on top of it shouldnâ€™t
be controversial.</p>
<p>When you add all of this up, it paints a compelling argument for a
shared stack. Yes, build and testing probably need to be done on
separate systems, but versioning, deployment, measurement and validation
all fit nicely with existing devops infrastructure.</p>
<h2 id="problems">Problems</h2>
<p>I admit that itâ€™s not all roses. Iâ€™ve already mentioned the
differences in the build and test steps. There are other differences,
too. Model building is very data-centric. Versioning your data (and
providing lineage) is something thatâ€™s not really modeled in a
traditional continuous deployment pipeline (though, static content
deployment is perhaps an analogue). <a
href="https://en.wikipedia.org/wiki/Lambda_architecture">Lambda</a>-style,
or realtime model training also donâ€™t map cleanly to a continuous
deployment flow. Still, I argue that there exists a large enough overlap
to warrant a shared stack for most of the infrastructure, as I outline
above.</p>
<h2 id="the-future">The future</h2>
<p>There are signs of hope in the devops and machine learning spaces.
GitLabâ€™s site has an interesting issue entitled <a
href="https://gitlab.com/gitlab-org/gitlab-ce/issues/46161">GitLab/Devops
for AI/ML</a>. Gitlab does a fantastic job modeling continuous
integration and delivery, and Iâ€™d be excited to progress in this area.
There are <a
href="https://towardsdatascience.com/deploying-machine-learning-models-with-docker-5d22a4dacb5">a
lot</a> of <a
href="https://www.udemy.com/deploy-data-science-nlp-models-with-docker-containers/">posts</a>
about <a
href="http://shop.oreilly.com/product/0636920084334.do">deploying
models</a> through <a
href="https://medium.com/analytics-vidhya/how-to-deploy-machine-learning-models-using-flask-docker-and-google-cloud-platform-gcp-6e7bf1b339d5">Docker</a>,
<a
href="https://towardsdatascience.com/version-control-for-jupyter-notebook-3e6cef13392d">version
control in JupyterHub</a>, and so on. Iâ€™m also pretty excited about how
<a href="https://www.prefect.io/">Prefect</a> is <a
href="https://medium.com/the-prefect-blog/positive-and-negative-data-engineering-a02cb497583d">thinking
about problems like this</a>.</p>
<p>In the meantime, the best path forward is probably to be
opportunistic about shared infrastructure. If you find yourself
deploying pre-built models, ask why they canâ€™t be deployed via the same
repository as your microservices. Likewise, when measuring the
performance of a model in production, why not see how easy itâ€™d be to
use your existing metrics and monitoring infrastructure?</p>
</div>
<form style="background-color: #eee; padding:1em; text-align:center;" action="https://tinyletter.com/criccomini" method="post" target="popupwindow" onsubmit="window.open('https://tinyletter.com/criccomini', 'popupwindow', 'scrollbars=yes,width=800,height=600');return true">
<h1><label for="tlemail">Never Miss a Post</label></h1>
<span class="by">Subscribe to my newsletter!</span>
<p style="margin-bottom: .35em;">
<input type="text" style="width:140px" name="email" id="tlemail" placeholder="your@email.com" />
<input type="hidden" value="1" name="embed"/><input type="submit" value="Subscribe" />
</p>
</form></div>
</body>
</html>